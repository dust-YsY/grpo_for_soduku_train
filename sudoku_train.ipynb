{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-13 18:13:31 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ä½¿ç”¨unslothæ¡†æ¶å¾®è°ƒQwen2.5-7B-Instructæ¨¡å‹æ¥è§£å†³æ•°ç‹¬é—®é¢˜\n",
    "è¿™ä¸ªè„šæœ¬åŒ…å«äº†æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„å®Œæ•´è¿‡ç¨‹\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"sudoku_solving_qwen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¨¡å‹å‚æ•°\n",
    "max_seq_length = 3000  # å¯ä»¥å¢åŠ ä»¥é€‚åº”æ›´é•¿çš„æ¨ç†è¿‡ç¨‹\n",
    "lora_rank = 16  # æ›´å¤§çš„rank = æ›´æ™ºèƒ½ï¼Œä½†æ›´æ…¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_path, train_ratio=0.9):\n",
    "    \"\"\"åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®\"\"\"\n",
    "    \n",
    "    # åŠ è½½æ•°æ®é›†\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sudoku_dataset = json.load(f)\n",
    "    \n",
    "    print(f\"åŠ è½½äº† {len(sudoku_dataset)} æ¡æ•°ç‹¬æ•°æ®\")\n",
    "    \n",
    "    # æ„å»ºè®­ç»ƒæ•°æ®é›†\n",
    "    training_data = []\n",
    "\n",
    "        # æ·»åŠ ç³»ç»Ÿæç¤º\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜:\n",
    "    <think>æ¨ç†è¿‡ç¨‹</think>\n",
    "    <answer>ç­”æ¡ˆ</answer>\n",
    "    \"\"\"\n",
    "    \n",
    "    for example in sudoku_dataset:\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answer\"]\n",
    "        \n",
    "        # æ„å»ºQwen2.5çš„è¾“å…¥æ ¼å¼\n",
    "        prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        full_prompt = prompt + answer + \"<|im_end|>\"\n",
    "        \n",
    "        training_data.append({\"text\": full_prompt})\n",
    "    \n",
    "    # è½¬æ¢ä¸ºHuggingFaceæ•°æ®é›†æ ¼å¼\n",
    "    random.shuffle(training_data)  # éšæœºæ‰“ä¹±æ•°æ®\n",
    "    train_size = int(len(training_data) * train_ratio)  \n",
    "    train_dataset = Dataset.from_list(training_data[:train_size])\n",
    "    eval_dataset = Dataset.from_list(training_data[train_size:])\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "    print(f\"éªŒè¯é›†å¤§å°: {len(eval_dataset)}\")\n",
    "    \n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=\"Qwen/Qwen2.5-3B-Instruct\"):\n",
    "    \"\"\"åŠ è½½åŸºç¡€æ¨¡å‹å¹¶é…ç½®LoRA\"\"\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ], # Remove QKVO if out of memory\n",
    "        lora_alpha = lora_rank,\n",
    "        use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "def setup_trainer(model, tokenizer, train_dataset):\n",
    "    \"\"\"è®¾ç½®è®­ç»ƒå™¨\"\"\"\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = train_dataset,\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_num_proc = 2,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = 2,\n",
    "            gradient_accumulation_steps = 4,\n",
    "            warmup_steps = 5,\n",
    "            # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "            max_steps = 60,\n",
    "            learning_rate = 2e-4,\n",
    "            fp16 = not is_bfloat16_supported(),\n",
    "            bf16 = is_bfloat16_supported(),\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.01,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 3407,\n",
    "            output_dir = \"outputs\",\n",
    "            report_to = \"none\", # Use this for WandB etc\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer, test_question):\n",
    "    \"\"\"æµ‹è¯•æ¨¡å‹åœ¨ç»™å®šé—®é¢˜ä¸Šçš„è¡¨ç°\"\"\"\n",
    "\n",
    "    # æ·»åŠ ç³»ç»Ÿæç¤º\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜:\n",
    "    <think>æ¨ç†è¿‡ç¨‹</think>\n",
    "    <answer>ç­”æ¡ˆ</answer>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{test_question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    # alpaca_prompt = Copied from above\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 3000, use_cache = True)\n",
    "    generated_text = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    # æå–ç”Ÿæˆæ–‡æœ¬ä¸­çš„åŠ©æ‰‹å›å¤éƒ¨åˆ†\n",
    "    assistant_response = generated_text[0].split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0]\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, num_samples=5):\n",
    "    \"\"\"è¯„ä¼°æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨æ˜¯å¦ç”Ÿæˆäº†<think>å’Œ<answer>æ ‡ç­¾\"\"\"\n",
    "\n",
    "    # æ·»åŠ ç³»ç»Ÿæç¤º\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜:\n",
    "    <think>æ¨ç†è¿‡ç¨‹</think>\n",
    "    <answer>ç­”æ¡ˆ</answer>\n",
    "    \"\"\"\n",
    "    \n",
    "    # éšæœºé€‰æ‹©æ ·æœ¬\n",
    "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices:\n",
    "        example = dataset[idx]\n",
    "        text = example[\"text\"]\n",
    "        \n",
    "        # æå–é—®é¢˜\n",
    "        question = text.split(\"<|im_start|>user\\n\")[1].split(\"<|im_end|>\")[0]\n",
    "        prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        \n",
    "        # alpaca_prompt = Copied from above\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "        inputs = tokenizer(\n",
    "        [\n",
    "            prompt\n",
    "        ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 3000, use_cache = True)\n",
    "        generated_text = tokenizer.batch_decode(outputs)\n",
    "        \n",
    "        # æå–ç­”æ¡ˆ\n",
    "        answer = generated_text[0].split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0]\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦åŒ…å« <think> å’Œ <answer> æ ‡è®°\n",
    "        has_think_tag = \"<think>\" in answer and \"</think>\" in answer\n",
    "        has_answer_tag = \"<answer>\" in answer and \"</answer>\" in answer\n",
    "        \n",
    "        # æ”¶é›†ç»“æœ\n",
    "        results.append({\n",
    "            \"has_think_tag\": has_think_tag,\n",
    "            \"has_answer_tag\": has_answer_tag,\n",
    "            \"answer\": answer,\n",
    "        })\n",
    "    \n",
    "    # è®¡ç®—ç»Ÿè®¡æ•°æ®\n",
    "    stats = {\n",
    "        \"total_samples\": len(results),\n",
    "        \"samples_with_think_tag\": sum(1 for r in results if r[\"has_think_tag\"]),\n",
    "        \"samples_with_answer_tag\": sum(1 for r in results if r[\"has_answer_tag\"]),\n",
    "        \"samples_with_both_tags\": sum(1 for r in results if r[\"has_think_tag\"] and r[\"has_answer_tag\"]),\n",
    "    }\n",
    "\n",
    "    return results, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(original_model, original_tokenizer, finetuned_model, finetuned_tokenizer, test_question):\n",
    "    \n",
    "    # ä½¿ç”¨åŸå§‹æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ\n",
    "    original_answer = test_model(original_model, original_tokenizer, test_question)\n",
    "    \n",
    "    # ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ\n",
    "    finetuned_answer = test_model(finetuned_model, finetuned_tokenizer, test_question)\n",
    "    \n",
    "    return original_answer, finetuned_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½äº† 50 æ¡æ•°ç‹¬æ•°æ®\n",
      "è®­ç»ƒé›†å¤§å°: 45\n",
      "éªŒè¯é›†å¤§å°: 5\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½å’Œå‡†å¤‡æ•°æ®\n",
    "train_dataset, eval_dataset = load_and_prepare_data(\"reasoning_sft_dataset/sudoku_reasoning_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•åŸå§‹æ¨¡å‹\n",
    "if len(eval_dataset) > 0:\n",
    "    test_example = eval_dataset[3][\"text\"]\n",
    "    test_question = test_example.split(\"<|im_start|>user\\n\")[1].split(\"<|im_end|>\")[0]\n",
    "    original_response = test_model(model, tokenizer, test_question)\n",
    "    print(\"é—®é¢˜ï¼š\")\n",
    "    print(test_question)\n",
    "    print(\"åŸå§‹æ¨¡å‹å›ç­”ç¤ºä¾‹:\")\n",
    "    print(original_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # è®¾ç½®è®­ç»ƒå™¨\n",
    "# trainer = setup_trainer(model, tokenizer, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å¼€å§‹è®­ç»ƒ\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ä¿å­˜æ¨¡å‹\n",
    "output_dir = \"output/sudoku_solving_qwen3b_sft\"\n",
    "# trainer.save_model(output_dir)\n",
    "# print(f\"æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  åŠ è½½å¾®è°ƒåçš„æ¨¡å‹\n",
    "print(\"\\næ­¥éª¤7: åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...\")\n",
    "finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=output_dir,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹\n",
    "print(\"\\næ­¥éª¤8: æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...\")\n",
    "if len(eval_dataset) > 0:\n",
    "    test_example = eval_dataset[3][\"text\"]\n",
    "    test_question = test_example.split(\"<|im_start|>user\\n\")[1].split(\"<|im_end|>\")[0]\n",
    "    finetuned_response = test_model(finetuned_model, finetuned_tokenizer, test_question)\n",
    "    print(\"é—®é¢˜ï¼š\")\n",
    "    print(test_question)\n",
    "    print(\"sftæ¨¡å‹å›ç­”ç¤ºä¾‹:\")\n",
    "    print(finetuned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°åŠ è½½åŸå§‹æ¨¡å‹è¿›è¡Œè¯„ä¼°\n",
    "print(\"åŠ è½½åŸå§‹æ¨¡å‹è¿›è¡Œè¯„ä¼°...\")\n",
    "original_model, original_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = True,\n",
    "    gpu_memory_utilization = 0.8,\n",
    ")\n",
    "\n",
    "original_results, original_stats = evaluate_model(original_model, original_tokenizer, eval_dataset, num_samples=1)\n",
    "print(\"åŸå§‹æ¨¡å‹è¯„ä¼°ç»“æœ:\")\n",
    "print(\"åŒ…å«<think>æ ‡ç­¾çš„æ ·æœ¬æ•°:\", original_stats[\"samples_with_think_tag\"])\n",
    "print(\"åŒ…å«<answer>æ ‡ç­¾çš„æ ·æœ¬æ•°:\", original_stats[\"samples_with_answer_tag\"])\n",
    "print(\"åŒæ—¶åŒ…å«ä¸¤ä¸ªæ ‡ç­¾çš„æ ·æœ¬æ•°:\", original_stats[\"samples_with_both_tags\"])\n",
    "print(\"\\nåŸå§‹æ¨¡å‹å›ç­”ç¤ºä¾‹:\")\n",
    "print(original_results[0][\"answer\"])\n",
    "\n",
    "print(\"\\nè¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹...\")\n",
    "finetuned_results, finetuned_stats = evaluate_model(finetuned_model, finetuned_tokenizer, eval_dataset, num_samples=1)\n",
    "print(\"å¾®è°ƒåæ¨¡å‹è¯„ä¼°ç»“æœ:\")\n",
    "print(\"åŒ…å«<think>æ ‡ç­¾çš„æ ·æœ¬æ•°:\", finetuned_stats[\"samples_with_think_tag\"])\n",
    "print(\"åŒ…å«<answer>æ ‡ç­¾çš„æ ·æœ¬æ•°:\", finetuned_stats[\"samples_with_answer_tag\"])\n",
    "print(\"åŒæ—¶åŒ…å«ä¸¤ä¸ªæ ‡ç­¾çš„æ ·æœ¬æ•°:\", finetuned_stats[\"samples_with_both_tags\"])\n",
    "print(\"\\nå¾®è°ƒåæ¨¡å‹å›ç­”ç¤ºä¾‹:\")\n",
    "print(finetuned_results[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥éª¤10: åˆ›å»ºä¸€ä¸ªæ–°çš„æµ‹è¯•æ•°ç‹¬\n",
    "new_sudoku = \"\"\"ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ•°ç‹¬æ¸¸æˆï¼Œåœ¨9ä¹˜9çš„81å®«æ ¼ä¸­ï¼Œæ•°å­—çš„é¡ºåºåˆ†åˆ«ä¸ºï¼š\n",
    "8 6 3 | 2 0 4 | 9 1 5\n",
    "1 5 9 | 6 3 8 | 7 4 2\n",
    "4 2 7 | 5 9 1 | 3 8 6\n",
    "------+-------+------\n",
    "9 1 6 | 8 2 3 | 5 7 4\n",
    "7 4 5 | 1 6 9 | 2 3 8\n",
    "3 8 2 | 4 5 7 | 6 9 1\n",
    "------+-------+------\n",
    "6 0 8 | 3 4 2 | 1 5 7\n",
    "5 7 1 | 9 8 6 | 4 2 3\n",
    "2 3 4 | 7 1 5 | 8 6 9\n",
    "å…¶ä¸­0ä»£è¡¨ç©ºç¼ºçš„æ•°å­—ï¼Œéœ€è¦ä½ å»å¡«å†™ï¼Œè¯·ä½ å®Œæˆè¿™ä¸ªæ•°ç‹¬æ¸¸æˆï¼Œå¹¶è¾“å‡ºç›¸åŒæ ¼å¼çš„ç­”æ¡ˆã€‚\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "original_new_answer, finetuned_new_answer = compare_models(\n",
    "    model, tokenizer, finetuned_model, finetuned_tokenizer, new_sudoku\n",
    ")\n",
    "\n",
    "print(\"æ–°çš„æ•°ç‹¬é—®é¢˜:\")\n",
    "print(new_sudoku)\n",
    "print(\"\\nåŸå§‹æ¨¡å‹å›ç­”:\")\n",
    "print(original_new_answer[:3000] + \"...\" if len(original_new_answer) > 3000 else original_new_answer)\n",
    "\n",
    "\n",
    "finetuned_new_answer = test_model(finetuned_model, finetuned_tokenizer, test_question)\n",
    "\n",
    "print(\"\\nå¾®è°ƒåæ¨¡å‹å›ç­”:\")\n",
    "print(finetuned_new_answer[:3000] + \"...\" if len(finetuned_new_answer) > 3000 else finetuned_new_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def format_sudoku(puzzle_str):\n",
    "    \"\"\"å°†81ä¸ªå­—ç¬¦çš„æ•°ç‹¬å­—ç¬¦ä¸²è½¬æ¢ä¸º9x9çŸ©é˜µæ ¼å¼\"\"\"\n",
    "    if len(puzzle_str) != 81:\n",
    "        print(f\"è­¦å‘Š: æ•°ç‹¬å­—ç¬¦ä¸²é•¿åº¦ä¸æ˜¯81ï¼Œè€Œæ˜¯{len(puzzle_str)}ï¼Œæ— æ³•æ ¼å¼åŒ–ã€‚\")\n",
    "        return None\n",
    "\n",
    "    grid = np.array(list(puzzle_str)).reshape(9, 9)\n",
    "    formatted = []\n",
    "    for i in range(9):\n",
    "        row = \" \".join(grid[i, :3]) + \" | \" + \" \".join(grid[i, 3:6]) + \" | \" + \" \".join(grid[i, 6:])\n",
    "        formatted.append(row)\n",
    "        if i == 2 or i == 5:\n",
    "            formatted.append(\"------+-------+------\")\n",
    "    \n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "def get_sudoku_dataset(split=\"train\") -> Dataset:\n",
    "    \"\"\"åŠ è½½æ•°ç‹¬æ•°æ®é›†å¹¶è½¬æ¢ä¸ºGRPOè®­ç»ƒæ‰€éœ€çš„æ ¼å¼\"\"\"\n",
    "    # è¯»å–CSVæ–‡ä»¶\n",
    "    df = pd.read_csv(\"dataset/sudoku_cluewise.csv\")\n",
    "    \n",
    "    # ç­›é€‰çº¿ç´¢æ•°é‡å¤§äºç­‰äº78çš„æ•°æ®\n",
    "    df = df[df['clue_numbers'] >= 78]\n",
    "    \n",
    "    # å¦‚æœæ•°æ®é‡è¶…è¿‡500ï¼Œéšæœºé€‰æ‹©500æ¡\n",
    "    if len(df) > 500:\n",
    "        df = df.sample(n=500, random_state=42)\n",
    "    \n",
    "    print(f\"ç­›é€‰åæ•°æ®é›†å¤§å°: {len(df)}\")\n",
    "    \n",
    "    # è½¬æ¢æ•°æ®æ ¼å¼\n",
    "    def transform_data(row):\n",
    "        puzzle = row['quizzes']\n",
    "        solution = row['solutions']\n",
    "        \n",
    "        # æ ¼å¼åŒ–æ•°ç‹¬è°œé¢˜\n",
    "        formatted_puzzle = format_sudoku(puzzle)\n",
    "        if formatted_puzzle is None:\n",
    "            return None\n",
    "            \n",
    "        # æ„å»ºé—®é¢˜\n",
    "        question = f\"ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ•°ç‹¬æ¸¸æˆï¼Œåœ¨9ä¹˜9çš„81å®«æ ¼ä¸­ï¼Œæ•°å­—çš„é¡ºåºåˆ†åˆ«ä¸ºï¼š\\n{formatted_puzzle}\\nå…¶ä¸­0ä»£è¡¨ç©ºç¼ºçš„æ•°å­—ï¼Œéœ€è¦ä½ å»å¡«å†™ï¼Œè¯·ä½ å®Œæˆè¿™ä¸ªæ•°ç‹¬æ¸¸æˆï¼Œå¹¶è¾“å‡ºç›¸åŒæ ¼å¼çš„ç­”æ¡ˆã€‚\"\n",
    "\n",
    "        # æ·»åŠ ç³»ç»Ÿæç¤º\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜:\n",
    "        <think>æ¨ç†è¿‡ç¨‹</think>\n",
    "        <answer>ç­”æ¡ˆ</answer>\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\n",
    "            'prompt': [\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            'answer': format_sudoku(solution)\n",
    "        }\n",
    "    \n",
    "    # è½¬æ¢æ•°æ®\n",
    "    transformed_data = [transform_data(row) for _, row in df.iterrows()]\n",
    "    transformed_data = [x for x in transformed_data if x is not None]\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDatasetæ ¼å¼\n",
    "    dataset = Dataset.from_list(transformed_data)\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "    if split == \"train\":\n",
    "        dataset = dataset.select(range(int(0.9 * len(dataset))))\n",
    "    else:\n",
    "        dataset = dataset.select(range(int(0.9 * len(dataset)), len(dataset)))\n",
    "    \n",
    "    print(f\"{split}é›†å¤§å°: {len(dataset)}\")\n",
    "    return dataset\n",
    "\n",
    "# æå–<answer>æ ‡ç­¾ä¸­çš„å†…å®¹\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    if \"<answer>\" not in text or \"</answer>\" not in text:\n",
    "        return \"\"\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "# æ­£ç¡®æ€§å¥–åŠ±å‡½æ•°\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    \n",
    "    # æå–ç­”æ¡ˆï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    \n",
    "    # print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    \n",
    "    # æ¯”è¾ƒç­”æ¡ˆï¼Œç©ºå­—ç¬¦ä¸²ç›´æ¥è¿”å›0åˆ†\n",
    "    return [2.0 if r and r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "# è½¯æ ¼å¼å¥–åŠ±å‡½æ•°\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    print(\"\\n=== Soft Format Reward Debug ===\")\n",
    "    for i, response in enumerate(responses):\n",
    "        contains_think_open = \"<think>\" in response\n",
    "        contains_think_close = \"</think>\" in response\n",
    "        contains_answer_open = \"<answer>\" in response\n",
    "        contains_answer_close = \"</answer>\" in response\n",
    "        \n",
    "        match = re.match(pattern, response, re.DOTALL)\n",
    "        \n",
    "        # è®¡ç®—å¥–åŠ±\n",
    "        if match:\n",
    "            reward = 1.0  # å®Œå…¨åŒ¹é…æ­£åˆ™ï¼Œç›´æ¥ç»™æ»¡åˆ†\n",
    "        else:\n",
    "            reward = (\n",
    "                (0.15 if contains_think_open else 0) +\n",
    "                (0.15 if contains_think_close else 0) +\n",
    "                (0.15 if contains_answer_open else 0) +\n",
    "                (0.15 if contains_answer_close else 0)\n",
    "            )\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # è°ƒè¯•ä¿¡æ¯\n",
    "        print(f\"\\nResponse {i}:\")\n",
    "        print(f\"Contains <think>: {contains_think_open}\")\n",
    "        print(f\"Contains </think>: {contains_think_close}\")\n",
    "        print(f\"Contains <answer>: {contains_answer_open}\")\n",
    "        print(f\"Contains </answer>: {contains_answer_close}\")\n",
    "        print(f\"Pattern match: {match}\")\n",
    "        print(f\"Final Reward: {reward}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# æ ¼å¼æ­£ç¡®æ€§å¥–åŠ±å‡½æ•°\n",
    "def format_correctness_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the answer follows the correct format.\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    \n",
    "    print(\"\\n=== Format Correctness Reward Debug ===\")\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"\\nResponse {i}:\")\n",
    "        # æå–<answer>æ ‡ç­¾ä¸­çš„å†…å®¹\n",
    "        answer = extract_xml_answer(response)\n",
    "        \n",
    "        # å¦‚æœæå–å¤±è´¥ï¼Œç›´æ¥è¿”å›0åˆ†\n",
    "        if not answer:\n",
    "            print(\"Failed to extract answer from XML tags\")\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        print(\"Extracted answer:\")\n",
    "        print(answer)\n",
    "        \n",
    "        # æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®\n",
    "        lines = answer.strip().split('\\n')\n",
    "        print(f\"Number of lines: {len(lines)}\")\n",
    "        \n",
    "        if len(lines) != 11:  # 9è¡Œæ•°å­— + 2è¡Œåˆ†éš”çº¿\n",
    "            print(f\"Wrong number of lines: expected 11, got {len(lines)}\")\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        # æ£€æŸ¥æ¯è¡Œçš„æ ¼å¼\n",
    "        format_correct = True\n",
    "        for i, line in enumerate(lines):\n",
    "            if i in [3, 7]:  # åˆ†éš”çº¿\n",
    "                if line != \"------+-------+------\":\n",
    "                    print(f\"Wrong separator line at index {i}: {line}\")\n",
    "                    format_correct = False\n",
    "                    break\n",
    "            else:  # æ•°å­—è¡Œ\n",
    "                parts = line.split('|')\n",
    "                if len(parts) != 3:\n",
    "                    print(f\"Wrong number of parts at line {i}: {line}\")\n",
    "                    format_correct = False\n",
    "                    break\n",
    "                for j, part in enumerate(parts):\n",
    "                    if len(part.strip().split()) != 3:\n",
    "                        print(f\"Wrong number of numbers in part {j} of line {i}: {part}\")\n",
    "                        format_correct = False\n",
    "                        break\n",
    "        \n",
    "        print(f\"Format correct: {format_correct}\")\n",
    "        rewards.append(1.0 if format_correct else 0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "def sudoku_validity_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the answer is a valid Sudoku solution.\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    \n",
    "    print(\"\\n=== Sudoku Validity Reward Debug ===\")\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"\\nResponse {i}:\")\n",
    "        # æå–<answer>æ ‡ç­¾ä¸­çš„å†…å®¹\n",
    "        answer = extract_xml_answer(response)\n",
    "        \n",
    "        # å¦‚æœæå–å¤±è´¥ï¼Œç›´æ¥è¿”å›0åˆ†\n",
    "        if not answer:\n",
    "            print(\"Failed to extract answer from XML tags\")\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # å°†ç­”æ¡ˆè½¬æ¢ä¸º9x9çŸ©é˜µ\n",
    "            lines = answer.strip().split('\\n')\n",
    "            grid = []\n",
    "            for line in lines:\n",
    "                if line == \"------+-------+------\":\n",
    "                    continue\n",
    "                # ç§»é™¤åˆ†éš”ç¬¦å¹¶åˆ†å‰²æ•°å­—\n",
    "                numbers = line.replace('|', '').split()\n",
    "                grid.append([int(n) for n in numbers])\n",
    "            \n",
    "            if len(grid) != 9 or any(len(row) != 9 for row in grid):\n",
    "                print(\"Invalid grid dimensions\")\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            # æ£€æŸ¥æ¯è¡Œ\n",
    "            row_rewards = 0\n",
    "            for row_idx, row in enumerate(grid):\n",
    "                if set(row) == set(range(1, 10)):\n",
    "                    row_rewards += 0.1\n",
    "                    print(f\"Row {row_idx + 1} is valid\")\n",
    "                else:\n",
    "                    print(f\"Row {row_idx + 1} is invalid: {row}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ¯åˆ—\n",
    "            col_rewards = 0\n",
    "            for col in range(9):\n",
    "                column = [grid[row][col] for row in range(9)]\n",
    "                if set(column) == set(range(1, 10)):\n",
    "                    col_rewards += 0.1\n",
    "                    print(f\"Column {col + 1} is valid\")\n",
    "                else:\n",
    "                    print(f\"Column {col + 1} is invalid: {column}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ¯ä¸ª3x3å°æ¡†\n",
    "            box_rewards = 0\n",
    "            for box_row in range(0, 9, 3):\n",
    "                for box_col in range(0, 9, 3):\n",
    "                    # æå–3x3å°æ¡†ä¸­çš„æ•°å­—\n",
    "                    box = []\n",
    "                    for i in range(3):\n",
    "                        for j in range(3):\n",
    "                            box.append(grid[box_row + i][box_col + j])\n",
    "                    if set(box) == set(range(1, 10)):\n",
    "                        box_rewards += 0.1\n",
    "                        print(f\"Box at ({box_row}, {box_col}) is valid\")\n",
    "                    else:\n",
    "                        print(f\"Box at ({box_row}, {box_col}) is invalid: {box}\")\n",
    "            \n",
    "            # æ€»å¥–åŠ±ä¸ºè¡Œå¥–åŠ±ã€åˆ—å¥–åŠ±å’Œå°æ¡†å¥–åŠ±ä¹‹å’Œ\n",
    "            total_reward = row_rewards + col_rewards + box_rewards\n",
    "            print(f\"Total reward: {total_reward} (rows: {row_rewards}, columns: {col_rewards}, boxes: {box_rewards})\")\n",
    "            rewards.append(total_reward)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing grid: {str(e)}\")\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# çº¿ç´¢ä¿ç•™å’Œç©ºå•å…ƒæ ¼å¡«å……å¥–åŠ±å‡½æ•°\n",
    "def clue_preservation_reward_func(prompts, completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if original clues are preserved and rewards correct empty cell filling.\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    questions = [prompt[-1][\"content\"] for prompt in prompts]\n",
    "    rewards = []\n",
    "    \n",
    "    print(\"\\n=== Clue Preservation and Empty Cell Reward Debug ===\")\n",
    "    for i, (response, question) in enumerate(zip(responses, questions)):\n",
    "        print(f\"\\nResponse {i}:\")\n",
    "        \n",
    "        # æå–ç­”æ¡ˆ\n",
    "        answer = extract_xml_answer(response)\n",
    "        if not answer:\n",
    "            print(\"Failed to extract answer from XML tags\")\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # ä»é—®é¢˜ä¸­æå–åŸå§‹æ•°ç‹¬\n",
    "            original_grid = []\n",
    "            for line in question.split('\\n'):\n",
    "                if '|' in line and not line.startswith('------'):\n",
    "                    # ç§»é™¤åˆ†éš”ç¬¦å¹¶åˆ†å‰²æ•°å­—\n",
    "                    numbers = line.replace('|', '').split()\n",
    "                    original_grid.append([int(n) if n != '0' else 0 for n in numbers])\n",
    "            \n",
    "            # ä»ç­”æ¡ˆä¸­æå–å¡«å……åçš„æ•°ç‹¬\n",
    "            filled_grid = []\n",
    "            for line in answer.split('\\n'):\n",
    "                if '|' in line and not line.startswith('------'):\n",
    "                    # ç§»é™¤åˆ†éš”ç¬¦å¹¶åˆ†å‰²æ•°å­—\n",
    "                    numbers = line.replace('|', '').split()\n",
    "                    filled_grid.append([int(n) for n in numbers])\n",
    "            \n",
    "            # æ£€æŸ¥åŸå§‹çº¿ç´¢æ˜¯å¦ä¿æŒä¸å˜\n",
    "            clue_preserved = True\n",
    "            empty_cells = 0\n",
    "            correct_fills = 0\n",
    "            \n",
    "            for i in range(9):\n",
    "                for j in range(9):\n",
    "                    if original_grid[i][j] != 0:  # è¿™æ˜¯ä¸€ä¸ªåŸå§‹çº¿ç´¢\n",
    "                        if original_grid[i][j] != filled_grid[i][j]:\n",
    "                            clue_preserved = False\n",
    "                            print(f\"Original clue changed at position ({i}, {j}): {original_grid[i][j]} -> {filled_grid[i][j]}\")\n",
    "                    else:  # è¿™æ˜¯ä¸€ä¸ªç©ºå•å…ƒæ ¼\n",
    "                        empty_cells += 1\n",
    "                        if filled_grid[i][j] in range(1, 10):  # ç¡®ä¿å¡«å……çš„æ˜¯æœ‰æ•ˆæ•°å­—\n",
    "                            correct_fills += 1\n",
    "            \n",
    "            if not clue_preserved:\n",
    "                print(\"Original clues were not preserved\")\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            # è®¡ç®—å¥–åŠ±\n",
    "            if empty_cells > 0:\n",
    "                reward = correct_fills / empty_cells\n",
    "                print(f\"Empty cells: {empty_cells}, Correct fills: {correct_fills}, Reward: {reward}\")\n",
    "            else:\n",
    "                reward = 1.0  # å¦‚æœæ²¡æœ‰ç©ºå•å…ƒæ ¼ï¼Œè¯´æ˜æ‰€æœ‰çº¿ç´¢éƒ½æ­£ç¡®\n",
    "                print(\"No empty cells to fill\")\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing grids: {str(e)}\")\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½è®­ç»ƒæ•°æ®\n",
    "train_dataset = get_sudoku_dataset(split=\"train\")\n",
    "eval_dataset = get_sudoku_dataset(split=\"eval\")\n",
    "\n",
    "# # åˆå§‹åŒ–wandb\n",
    "# wandb.init(\n",
    "#     project=\"sudoku_solving_qwen\",\n",
    "#     config={\n",
    "#         \"model_name\": \"Qwen2.5-0.5B-Instruct\",\n",
    "#         \"max_seq_length\": max_seq_length,\n",
    "#         \"lora_rank\": lora_rank,\n",
    "#         \"learning_rate\": 5e-6,\n",
    "#         \"batch_size\": 1,\n",
    "#         \"gradient_accumulation_steps\": 1,\n",
    "#         \"num_generations\": 6,\n",
    "#         \"max_steps\": 250,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# training_args = GRPOConfig(\n",
    "#     learning_rate = 5e-6,\n",
    "#     adam_beta1 = 0.9,\n",
    "#     adam_beta2 = 0.99,\n",
    "#     weight_decay = 0.1,\n",
    "#     warmup_ratio = 0.1,\n",
    "#     lr_scheduler_type = \"cosine\",\n",
    "#     optim = \"paged_adamw_8bit\",\n",
    "#     logging_steps = 1,\n",
    "#     per_device_train_batch_size = 1,\n",
    "#     gradient_accumulation_steps = 1,\n",
    "#     num_generations = 6,\n",
    "#     max_prompt_length = max_seq_length // 2,\n",
    "#     max_completion_length = max_seq_length // 2,\n",
    "#     max_steps = 500,\n",
    "#     save_steps = 250,\n",
    "#     max_grad_norm = 0.1,\n",
    "#     report_to = \"wandb\",  # å¯ç”¨wandbæŠ¥å‘Š\n",
    "#     output_dir = \"output/sudoku_solving_qwen3b_grpo\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„å›è°ƒç±»æ¥è®°å½•å¥–åŠ±å‡½æ•°çš„ç»“æœ\n",
    "from transformers import TrainerCallback\n",
    "class RewardCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.log_history:\n",
    "            # è®°å½•æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡\n",
    "            for log in state.log_history:\n",
    "                if isinstance(log, dict):\n",
    "                    wandb.log(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = GRPOTrainer(\n",
    "#     model = finetuned_model,\n",
    "#     processing_class = finetuned_tokenizer,\n",
    "#     reward_funcs = [\n",
    "#         soft_format_reward_func,\n",
    "#         correctness_reward_func,\n",
    "#         format_correctness_reward_func,\n",
    "#         sudoku_validity_reward_func,\n",
    "#         clue_preservation_reward_func,\n",
    "#     ],\n",
    "#     args = training_args,\n",
    "#     train_dataset = train_dataset,\n",
    "#     callbacks=[RewardCallback()],  # æ·»åŠ è‡ªå®šä¹‰å›è°ƒ\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å¼€å§‹è®­ç»ƒ\n",
    "# trainer.train()\n",
    "\n",
    "# # å…³é—­wandb\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ä¿å­˜æ¨¡å‹\n",
    "output_dir = \"output/sudoku_solving_qwen3b_grpo\"\n",
    "# trainer.save_model(output_dir)\n",
    "# print(f\"æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ­¥éª¤7: åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...\n",
      "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.50.0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2080 Ti. Num GPUs = 1. Max memory: 21.657 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 58.33%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 21.66 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 3000. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 6.5 GB. Also swap space = 4 GB.\n",
      "WARNING 04-13 18:17:00 [config.py:2599] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-13 18:17:03 [config.py:583] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 04-13 18:17:03 [arg_utils.py:1765] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-13 18:17:03 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=3000, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 04-13 18:17:04 [cuda.py:234] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-13 18:17:04 [cuda.py:282] Using XFormers backend.\n",
      "INFO 04-13 18:17:04 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-13 18:17:04 [model_runner.py:1110] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
      "INFO 04-13 18:17:04 [loader.py:1137] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 04-13 18:17:06 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae115754a3db4f5694f04019a4e9cc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb92bd237494472b64c0de9fb046ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-13 18:17:09 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-13 18:17:09 [model_runner.py:1146] Model loading took 7.0262 GB and 4.281014 seconds\n",
      "INFO 04-13 18:17:10 [worker.py:267] Memory profiling takes 1.54 seconds\n",
      "INFO 04-13 18:17:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (21.66GiB) x gpu_memory_utilization (0.58) = 12.63GiB\n",
      "INFO 04-13 18:17:10 [worker.py:267] model weights take 7.03GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.06GiB; the rest of the memory reserved for KV Cache is 4.52GiB.\n",
      "INFO 04-13 18:17:10 [executor_base.py:111] # cuda blocks: 5288, # CPU blocks: 4681\n",
      "INFO 04-13 18:17:10 [executor_base.py:116] Maximum concurrency for 3000 tokens per request: 28.20x\n",
      "INFO 04-13 18:17:12 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-13 18:17:24 [model_runner.py:1570] Graph capturing finished in 12 secs, took 0.55 GiB\n",
      "INFO 04-13 18:17:24 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 15.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.18 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "#  åŠ è½½å¾®è°ƒåçš„æ¨¡å‹\n",
    "print(\"\\næ­¥éª¤7: åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...\")\n",
    "finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=output_dir,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ­¥éª¤8: æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...\n",
      "é—®é¢˜ï¼š\n",
      "ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ•°ç‹¬æ¸¸æˆï¼Œåœ¨9ä¹˜9çš„81å®«æ ¼ä¸­ï¼Œæ•°å­—çš„é¡ºåºåˆ†åˆ«ä¸ºï¼š\n",
      "3 7 4 | 8 5 1 | 6 2 9\n",
      "1 5 8 | 6 9 2 | 4 3 7\n",
      "2 9 6 | 4 7 3 | 8 1 5\n",
      "------+-------+------\n",
      "8 1 7 | 5 2 6 | 9 4 3\n",
      "6 2 5 | 9 3 4 | 0 8 1\n",
      "4 3 9 | 7 1 8 | 5 6 2\n",
      "------+-------+------\n",
      "9 4 2 | 3 8 7 | 1 5 6\n",
      "5 6 1 | 2 4 9 | 3 7 8\n",
      "7 8 3 | 1 6 5 | 2 9 4\n",
      "å…¶ä¸­0ä»£è¡¨ç©ºç¼ºçš„æ•°å­—ï¼Œéœ€è¦ä½ å»å¡«å†™ï¼Œè¯·ä½ å®Œæˆè¿™ä¸ªæ•°ç‹¬æ¸¸æˆï¼Œå¹¶è¾“å‡ºç›¸åŒæ ¼å¼çš„ç­”æ¡ˆã€‚\n",
      "sftæ¨¡å‹å›ç­”ç¤ºä¾‹:\n",
      "<think>å—¯ï¼Œæˆ‘ç°åœ¨è¦è§£å†³è¿™ä¸ªæ•°ç‹¬é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ä»”ç»†è§‚å¯Ÿé¢˜ç›®ç»™å‡ºçš„æ•°ç‹¬å¸ƒå±€ï¼Œæ‰¾å‡ºæ‰€æœ‰0çš„ä½ç½®ï¼Œç„¶åæ ¹æ®æ•°ç‹¬çš„è§„åˆ™æ¥æ¨æ–­å‡ºæ­£ç¡®çš„æ•°å­—ã€‚æ•°ç‹¬çš„è§„åˆ™æ˜¯æ¯ä¸€è¡Œã€æ¯ä¸€åˆ—ä»¥åŠæ¯ä¸ª3x3çš„å°ä¹å®«æ ¼å†…æ•°å­—1-9ä¸èƒ½é‡å¤ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘å…ˆå°†é¢˜ç›®ä¸­çš„æ•°ç‹¬ç»“æ„æ•´ç†æ¸…æ¥šã€‚æ•°ç‹¬çš„ç»“æ„æ˜¯9x9çš„ï¼Œåˆ†ä¸º9ä¸ª3x3çš„å°å®«æ ¼ã€‚ç°åœ¨ï¼Œæˆ‘éœ€è¦æ‰¾å‡ºæ‰€æœ‰0çš„ä½ç½®ï¼Œå¹¶é€ä¸€è§£å†³ã€‚\n",
      "\n",
      "è§‚å¯Ÿç»™å‡ºçš„æ•°ç‹¬ï¼Œæˆ‘æ³¨æ„åˆ°ç¬¬6è¡Œçš„ç¬¬7åˆ—æœ‰ä¸€ä¸ª0ï¼Œä¹Ÿå°±æ˜¯åæ ‡ï¼ˆ6,7ï¼‰çš„ä½ç½®ã€‚å…¶ä»–ä½ç½®éƒ½æ˜¯å·²å¡«å¥½çš„æ•°å­—ã€‚ç°åœ¨ï¼Œæˆ‘éœ€è¦ç¡®å®šè¿™ä¸ª0åº”è¯¥å¡«ä»€ä¹ˆæ•°å­—ã€‚\n",
      "\n",
      "æ ¹æ®æ•°ç‹¬çš„è§„åˆ™ï¼Œæ¯ä¸€è¡Œã€åˆ—å’Œå°å®«æ ¼éƒ½å¿…é¡»åŒ…å«1-9çš„æ•°å­—ï¼Œä¸èƒ½é‡å¤ã€‚æ‰€ä»¥ï¼Œæˆ‘éœ€è¦æ£€æŸ¥ç¬¬6è¡Œã€ç¬¬7åˆ—ä»¥åŠç¬¬5-7è¡Œçš„ä¸­é—´å°å®«æ ¼ï¼ˆå³ç¬¬5-7è¡Œï¼Œç¬¬7-9åˆ—ï¼‰æ¥ç¡®å®š0çš„ä½ç½®åº”è¯¥å¡«ä»€ä¹ˆæ•°å­—ã€‚\n",
      "\n",
      "å…ˆçœ‹ç¬¬6è¡Œï¼Œå·²æœ‰çš„æ•°å­—æ˜¯4,3,9,7,1,8,5,6,2ï¼Œç¼ºå°‘çš„æ•°å­—æ˜¯0ï¼Œæ‰€ä»¥è¿™ä¸ªä½ç½®åº”è¯¥å¡«0ï¼Œä½†è¿™é‡Œå¯èƒ½æœ‰è¯¯ï¼Œå› ä¸º0çš„ä½ç½®åº”è¯¥å¡«å…¶ä»–æ•°å­—ã€‚æ‰€ä»¥ï¼Œæˆ‘éœ€è¦ä»”ç»†æ£€æŸ¥ã€‚\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘éœ€è¦æ£€æŸ¥ç¬¬6è¡Œçš„æ•°å­—ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰é‡å¤ã€‚å½“å‰ç¬¬6è¡Œçš„æ•°å­—æ˜¯4,3,9,7,1,8,0,5,6ã€‚æ‰€ä»¥ï¼Œç¼ºå°‘çš„æ•°å­—æ˜¯2ã€‚å› æ­¤ï¼Œç¬¬6è¡Œç¬¬7åˆ—çš„0åº”è¯¥å¡«2ã€‚è¿™æ ·ï¼Œç¬¬6è¡Œå°±å®Œæ•´äº†ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦æ£€æŸ¥ç¬¬7åˆ—æ˜¯å¦æœ‰å…¶ä»–æ•°å­—ï¼Œç¡®ä¿æ²¡æœ‰é‡å¤ã€‚ç¬¬7åˆ—çš„æ•°å­—æ˜¯6,4,8,9,0,5,1,3,2ã€‚è¿™é‡Œ0çš„ä½ç½®åœ¨ç¬¬6è¡Œï¼Œæ‰€ä»¥å¡«2æ˜¯æ­£ç¡®çš„ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦æ£€æŸ¥ç¬¬5-7è¡Œçš„ä¸­é—´å°å®«æ ¼ï¼ˆç¬¬5-7è¡Œï¼Œç¬¬7-9åˆ—ï¼‰æ˜¯å¦æœ‰é‡å¤ã€‚å½“å‰è¿™ä¸ªå°å®«æ ¼ä¸­çš„æ•°å­—æ˜¯0ï¼ˆå³ç¬¬6è¡Œç¬¬7åˆ—ï¼‰ã€8ã€1ï¼›ç¬¬7è¡Œç¬¬7åˆ—æ˜¯1ï¼Œç¬¬8è¡Œç¬¬7åˆ—æ˜¯3ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªå°å®«æ ¼ä¸­å·²æœ‰çš„æ•°å­—æ˜¯1,3,8,1,0,5,3,7,2ã€‚æ‰€ä»¥ï¼Œç¼ºå°‘çš„æ•°å­—æ˜¯4,6ã€‚ä½†å› ä¸ºç¬¬6è¡Œç¬¬7åˆ—å¡«2ï¼Œæ‰€ä»¥è¿™é‡Œæ²¡æœ‰é—®é¢˜ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œ0çš„ä½ç½®åº”è¯¥å¡«2ã€‚ç°åœ¨ï¼Œæ•´ä¸ªæ•°ç‹¬å°±å®Œæˆäº†ã€‚</think>\n",
      "\n",
      "<answer>3 7 4 | 8 5 1 | 6 2 9\n",
      "1 5 8 | 6 9 2 | 4 3 7\n",
      "2 9 6 | 4 7 3 | 8 1 5\n",
      "------+-------+------\n",
      "8 1 7 | 5 2 6 | 9 4 3\n",
      "6 2 5 | 9 3 4 | 7 8 1\n",
      "4 3 9 | 7 1 8 | 5 6 2\n",
      "------+-------+------\n",
      "9 4 2 | 3 8 7 | 1 5 6\n",
      "5 6 1 | 2 4 9 | 3 7 8\n",
      "7 8 3 | 1 6 5 | 2 9 4</answer>\n"
     ]
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()\n",
    "\n",
    "# æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹\n",
    "print(\"\\næ­¥éª¤8: æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...\")\n",
    "if len(eval_dataset) > 0:\n",
    "    test_example = eval_dataset[2][\"text\"]\n",
    "    test_question = test_example.split(\"<|im_start|>user\\n\")[1].split(\"<|im_end|>\")[0]\n",
    "    finetuned_response = test_model(finetuned_model, finetuned_tokenizer, test_question)\n",
    "    print(\"é—®é¢˜ï¼š\")\n",
    "    print(test_question)\n",
    "    print(\"grpoæ¨¡å‹å›ç­”ç¤ºä¾‹:\")\n",
    "    print(finetuned_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
